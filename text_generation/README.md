- Tokenize
    - sentence split
    - word split(delete stopwords)
    - tokenize
- train test set split
    - 2 million train set
    - first 2 sentences to last 1 sentence  
    - create 'text'-'target' data frame